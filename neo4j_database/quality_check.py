#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å›¾è°±è´¨é‡æ£€æŸ¥è„šæœ¬
- æ£€æŸ¥é‡å¤èŠ‚ç‚¹
- æ£€æŸ¥å­¤ç«‹èŠ‚ç‚¹
- æ£€æŸ¥å…³ç³»å®Œæ•´æ€§
"""

import os
import csv
from collections import defaultdict, Counter
from typing import Dict, List, Set


def check_duplicate_nodes(csv_dir: str) -> Dict[str, List]:
    """æ£€æŸ¥é‡å¤èŠ‚ç‚¹"""
    print("=" * 60)
    print("ğŸ” æ£€æŸ¥é‡å¤èŠ‚ç‚¹...")
    print("=" * 60)
    
    node_types = ['Paper', 'Task', 'ImagingModality', 'AnatomicalStructure',
                 'Method', 'Dataset', 'Metric', 'Innovation']
    
    duplicates = {}
    
    for node_type in node_types:
        csv_file = os.path.join(csv_dir, f'nodes_{node_type}.csv')
        
        if not os.path.exists(csv_file):
            continue
        
        # è¯»å–èŠ‚ç‚¹
        nodes = []
        name_to_ids = defaultdict(list)
        
        with open(csv_file, 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            for row in reader:
                nodes.append(row)
                
                # æ ¹æ®èŠ‚ç‚¹ç±»å‹é€‰æ‹©å”¯ä¸€æ ‡è¯†å­—æ®µ
                if node_type == 'Paper':
                    key = row.get('paper_id', '')
                elif node_type == 'Task':
                    key = row.get('name', '')
                elif node_type == 'ImagingModality':
                    key = row.get('name', '')
                elif node_type == 'AnatomicalStructure':
                    key = row.get('name', '')
                elif node_type == 'Method':
                    key = row.get('name', '')
                elif node_type == 'Dataset':
                    key = row.get('name', '')
                elif node_type == 'Metric':
                    key = row.get('name', '')
                elif node_type == 'Innovation':
                    key = row.get('description', '')
                else:
                    key = row.get('id', '')
                
                if key:
                    name_to_ids[key].append(row.get('id', ''))
        
        # æŸ¥æ‰¾é‡å¤
        node_duplicates = []
        for name, ids in name_to_ids.items():
            if len(ids) > 1:
                node_duplicates.append({
                    'name': name,
                    'ids': ids,
                    'count': len(ids)
                })
        
        if node_duplicates:
            duplicates[node_type] = node_duplicates
            print(f"\nâŒ {node_type}: å‘ç° {len(node_duplicates)} ä¸ªé‡å¤èŠ‚ç‚¹")
            for dup in node_duplicates[:10]:  # åªæ˜¾ç¤ºå‰10ä¸ª
                print(f"   - '{dup['name']}': {dup['count']} ä¸ªé‡å¤ (IDs: {dup['ids'][:3]}...)")
            if len(node_duplicates) > 10:
                print(f"   ... è¿˜æœ‰ {len(node_duplicates) - 10} ä¸ªé‡å¤èŠ‚ç‚¹")
        else:
            print(f"âœ… {node_type}: æ— é‡å¤èŠ‚ç‚¹ ({len(nodes)} ä¸ªèŠ‚ç‚¹)")
    
    return duplicates


def check_orphan_nodes(csv_dir: str) -> Dict[str, int]:
    """æ£€æŸ¥å­¤ç«‹èŠ‚ç‚¹ï¼ˆæ²¡æœ‰å…³ç³»çš„èŠ‚ç‚¹ï¼‰"""
    print("\n" + "=" * 60)
    print("ğŸ” æ£€æŸ¥å­¤ç«‹èŠ‚ç‚¹...")
    print("=" * 60)
    
    # è¯»å–æ‰€æœ‰å…³ç³»
    relations_file = os.path.join(csv_dir, 'relations.csv')
    if not os.path.exists(relations_file):
        print("âš  å…³ç³»æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡å­¤ç«‹èŠ‚ç‚¹æ£€æŸ¥")
        return {}
    
    connected_nodes = set()
    with open(relations_file, 'r', encoding='utf-8') as f:
        reader = csv.DictReader(f)
        for row in reader:
            connected_nodes.add(row.get('from_id', ''))
            connected_nodes.add(row.get('to_id', ''))
    
    # æ£€æŸ¥å„ç±»å‹èŠ‚ç‚¹
    node_types = ['Paper', 'Task', 'ImagingModality', 'AnatomicalStructure',
                 'Method', 'Dataset', 'Metric', 'Innovation']
    
    orphan_counts = {}
    
    for node_type in node_types:
        csv_file = os.path.join(csv_dir, f'nodes_{node_type}.csv')
        
        if not os.path.exists(csv_file):
            continue
        
        total_nodes = 0
        orphan_nodes = 0
        
        with open(csv_file, 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            for row in reader:
                total_nodes += 1
                node_id = row.get('id', '')
                if node_id and node_id not in connected_nodes:
                    orphan_nodes += 1
        
        orphan_counts[node_type] = {
            'total': total_nodes,
            'orphan': orphan_nodes,
            'connected': total_nodes - orphan_nodes
        }
        
        if orphan_nodes > 0:
            print(f"âš  {node_type}: {orphan_nodes}/{total_nodes} ä¸ªå­¤ç«‹èŠ‚ç‚¹ ({orphan_nodes/total_nodes*100:.1f}%)")
        else:
            print(f"âœ… {node_type}: æ‰€æœ‰èŠ‚ç‚¹éƒ½æœ‰è¿æ¥ ({total_nodes} ä¸ªèŠ‚ç‚¹)")
    
    return orphan_counts


def check_relation_integrity(csv_dir: str) -> Dict:
    """æ£€æŸ¥å…³ç³»å®Œæ•´æ€§"""
    print("\n" + "=" * 60)
    print("ğŸ” æ£€æŸ¥å…³ç³»å®Œæ•´æ€§...")
    print("=" * 60)
    
    relations_file = os.path.join(csv_dir, 'relations.csv')
    if not os.path.exists(relations_file):
        print("âš  å…³ç³»æ–‡ä»¶ä¸å­˜åœ¨")
        return {}
    
    # è¯»å–æ‰€æœ‰èŠ‚ç‚¹ID
    node_ids = set()
    node_types = ['Paper', 'Task', 'ImagingModality', 'AnatomicalStructure',
                 'Method', 'Dataset', 'Metric', 'Innovation']
    
    for node_type in node_types:
        csv_file = os.path.join(csv_dir, f'nodes_{node_type}.csv')
        if os.path.exists(csv_file):
            with open(csv_file, 'r', encoding='utf-8') as f:
                reader = csv.DictReader(f)
                for row in reader:
                    node_ids.add(row.get('id', ''))
    
    # æ£€æŸ¥å…³ç³»
    invalid_relations = []
    relation_types = Counter()
    
    with open(relations_file, 'r', encoding='utf-8') as f:
        reader = csv.DictReader(f)
        for row in reader:
            from_id = row.get('from_id', '')
            to_id = row.get('to_id', '')
            rel_type = row.get('type', '')
            
            relation_types[rel_type] += 1
            
            if from_id not in node_ids:
                invalid_relations.append({
                    'type': 'missing_from',
                    'from_id': from_id,
                    'to_id': to_id,
                    'rel_type': rel_type
                })
            if to_id not in node_ids:
                invalid_relations.append({
                    'type': 'missing_to',
                    'from_id': from_id,
                    'to_id': to_id,
                    'rel_type': rel_type
                })
    
    print(f"\nğŸ“Š å…³ç³»ç±»å‹ç»Ÿè®¡:")
    for rel_type, count in relation_types.most_common():
        print(f"   - {rel_type}: {count} æ¡")
    
    if invalid_relations:
        print(f"\nâŒ å‘ç° {len(invalid_relations)} æ¡æ— æ•ˆå…³ç³»:")
        missing_from = sum(1 for r in invalid_relations if r['type'] == 'missing_from')
        missing_to = sum(1 for r in invalid_relations if r['type'] == 'missing_to')
        print(f"   - ç¼ºå¤±èµ·å§‹èŠ‚ç‚¹: {missing_from} æ¡")
        print(f"   - ç¼ºå¤±ç›®æ ‡èŠ‚ç‚¹: {missing_to} æ¡")
    else:
        print(f"\nâœ… æ‰€æœ‰å…³ç³»éƒ½æœ‰æ•ˆ")
    
    return {
        'total_relations': sum(relation_types.values()),
        'relation_types': dict(relation_types),
        'invalid_relations': len(invalid_relations),
        'invalid_details': invalid_relations[:20]  # åªä¿å­˜å‰20ä¸ª
    }


def check_embedding_coverage(csv_dir: str) -> Dict[str, Dict]:
    """æ£€æŸ¥ embedding è¦†ç›–ç‡"""
    print("\n" + "=" * 60)
    print("ğŸ” æ£€æŸ¥ Embedding è¦†ç›–ç‡...")
    print("=" * 60)
    
    node_types = ['Paper', 'Task', 'ImagingModality', 'AnatomicalStructure',
                 'Method', 'Dataset', 'Metric', 'Innovation']
    
    embedding_stats = {}
    
    for node_type in node_types:
        csv_file = os.path.join(csv_dir, f'nodes_{node_type}.csv')
        
        if not os.path.exists(csv_file):
            continue
        
        total = 0
        with_embedding = 0
        empty_embedding = 0
        
        with open(csv_file, 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            for row in reader:
                total += 1
                embedding = row.get('embedding', '').strip()
                if embedding and embedding != '':
                    with_embedding += 1
                else:
                    empty_embedding += 1
        
        embedding_stats[node_type] = {
            'total': total,
            'with_embedding': with_embedding,
            'empty_embedding': empty_embedding,
            'coverage': with_embedding / total * 100 if total > 0 else 0
        }
        
        if empty_embedding > 0:
            print(f"âš  {node_type}: {with_embedding}/{total} ä¸ªèŠ‚ç‚¹æœ‰ embedding ({embedding_stats[node_type]['coverage']:.1f}%)")
        else:
            print(f"âœ… {node_type}: æ‰€æœ‰èŠ‚ç‚¹éƒ½æœ‰ embedding ({total} ä¸ªèŠ‚ç‚¹)")
    
    return embedding_stats


def generate_quality_report(csv_dir: str, output_file: str = None):
    """ç”Ÿæˆè´¨é‡æ£€æŸ¥æŠ¥å‘Š"""
    print("\n" + "=" * 60)
    print("ğŸ“‹ ç”Ÿæˆè´¨é‡æ£€æŸ¥æŠ¥å‘Š...")
    print("=" * 60)
    
    duplicates = check_duplicate_nodes(csv_dir)
    orphans = check_orphan_nodes(csv_dir)
    relations = check_relation_integrity(csv_dir)
    embeddings = check_embedding_coverage(csv_dir)
    
    # ç”ŸæˆæŠ¥å‘Š
    report = {
        'duplicates': duplicates,
        'orphans': orphans,
        'relations': relations,
        'embeddings': embeddings
    }
    
    if output_file:
        import json
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        print(f"\nâœ… æŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_file}")
    
    # æ€»ç»“
    print("\n" + "=" * 60)
    print("ğŸ“Š è´¨é‡æ£€æŸ¥æ€»ç»“")
    print("=" * 60)
    
    total_duplicates = sum(len(v) for v in duplicates.values())
    total_orphans = sum(v.get('orphan', 0) for v in orphans.values())
    invalid_rels = relations.get('invalid_relations', 0)
    
    if total_duplicates == 0 and total_orphans == 0 and invalid_rels == 0:
        print("âœ… å›¾è°±è´¨é‡è‰¯å¥½ï¼")
    else:
        if total_duplicates > 0:
            print(f"âš  å‘ç° {total_duplicates} ç»„é‡å¤èŠ‚ç‚¹")
        if total_orphans > 0:
            print(f"âš  å‘ç° {total_orphans} ä¸ªå­¤ç«‹èŠ‚ç‚¹")
        if invalid_rels > 0:
            print(f"âš  å‘ç° {invalid_rels} æ¡æ— æ•ˆå…³ç³»")
    
    return report


def main():
    """ä¸»å‡½æ•°"""
    script_dir = os.path.dirname(os.path.abspath(__file__))
    csv_dir = os.path.join(script_dir, 'csv')
    
    if not os.path.exists(csv_dir):
        print(f"âŒ CSV ç›®å½•ä¸å­˜åœ¨: {csv_dir}")
        print("   è¯·å…ˆè¿è¡Œ json_to_csv.py ç”Ÿæˆ CSV æ–‡ä»¶")
        return
    
    output_file = os.path.join(script_dir, 'quality_report.json')
    generate_quality_report(csv_dir, output_file)


if __name__ == '__main__':
    main()

